## 05/05/2023 / 14:00 - Notes ##
Why does it take longer to process a rising array
(that is, in theory, already sorted) than a randomized array?
a very good stackoverflow post explain this in good detail
https://stackoverflow.com/questions/11227809/why-is-processing-a-sorted-array-faster-than-processing-an-unsorted-array

Branch prediction failure:
    If you guess right: it continues on
    If you guess wrong: you have to slow down before going again

The quicksort algorithm we implemented has the following issue:
    if len(arr) <= 1:
        return arr

This is the first statement. In the case of a sorted array, our "right" array will never
be just one element or empty, while our left array will always be empty. so when we concatenate our left, right, and pivot,
we will end up moving the pivot one step, and then doing the concatenating again, only to move everything one step again.
Things don't really branch ouch in a seamless way.

branching is also generally bad for performance, and when we do two
recursions for each function call, in quicksort, that results in lots of branching. [1]

Results are similar for arrays holding multiples of only one value.

[1] Measuring the performance Impact of Branching instructions, Beierlieb, others


## 05/05/2023 / 14:45  - Notes ##

We need to plot elements on x, time taken on y. we have to plot our measured data
with deviation in mind, and also the function's expected big O curve.

x = np.array([1, 2, 3, 4, 5])
y = np.power(x, 2) # Effectively y = x**2
e = np.array([1.5, 2.6, 3.7, 4.6, 5.5])

plt.errorbar(x, y, e, linestyle='None', marker='^')

plt.show()



## 06/05/2023/ 09:21 - Notes ##
I found a short pdf on block-quicksort, i think its supposed to be like quicksort but with a smaller likelihood of branch prediction failure.
I'll continue reading it, ill probably download it too, just so that it is in my laptop's files.
"One of the most widely used sorting algorithms is the Quicksort, which has been by Hoare in 1962 and is considered
to be one of the most efficient sorting algorithms (...) Although its average number of comparisons is not optimal
- 138n log n + =(n) vs. n log n + O(n) for Mergesort -, its over-all instruction count is very low. Moreover, by
choosing the pivot element as median of some larger sample, the leading term 1.38n log n for the average number
of comparisons can be made smaller - Even down to n log n when choosing the pivot as median of some sample of
growing size. (...) A major drawback of Quicksort is its quadratic worst-case running time. Nevertheless, there are
efficient ways to circumvent a terrible worst-case scenario. (...) The most prominent is Intro-sort (introduced by
Musser) which is applied in GCC implementation of std::sort: As soon as recursion depth exceeds a certain limit, the
algorithm switches to Heapsort. "

Another deficiency of quicksort is that it suffers from branch misses in an essential way. On modern processors with
long pipelines, every missed branch prediction causes a rather long interruption of the execution since the pipeline has
to be filled anew.

Conditional jumps are the usual method of implementing sorting algorithms. These can be avoided.

## 09/05/2023 21:20 - Notes ##
I'm finally healthy enough to look at a screen without getting dizzy again! Sweet.
We have to make a function that will create arrays for the measure_time() function to eat up. This way we can keep our main clean.
Out of these functions, we're going to need to save the lowest and the highest measurements for a given configuration of N, algorithm, and array-type,
we will also save the median. So median, lowest measure, and high measure, are the most important things.
